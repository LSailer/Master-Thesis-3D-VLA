{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGGT Architecture Exploration\n",
    "\n",
    "**Goal:** Understand every component of VGGT (CVPR 2025 Best Paper) before integrating its geometry features into the 3D-VLA thesis pipeline.\n",
    "\n",
    "**Paper:** *Visual Geometry Grounded Transformer* — Facebook Research  \n",
    "**Model:** `facebook/VGGT-1B` (1B params)  \n",
    "**Kernel:** uv-managed Python 3.12 env (`uv run jupyter lab`)\n",
    "\n",
    "---\n",
    "## Table of Contents\n",
    "1. [Setup & Imports](#1)\n",
    "2. [Model Load & Parameter Counts](#2)\n",
    "3. [Load Example Images](#3)\n",
    "4. [Full Forward Pass](#4)\n",
    "5. [Depth & Point Cloud Visualization](#5)\n",
    "6. [DPT Head Deep-Dive](#6)\n",
    "7. [DPT Disabled Experiment](#7)\n",
    "8. [Camera Pose Encoding](#8)\n",
    "9. [Thesis Notes & Summary](#9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Imports <a id='1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.3.1\n",
      "device: mps\n",
      "VGGT on path: True\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "# Add VGGT source to path\n",
    "REPO_ROOT = os.path.abspath(\"..\")\n",
    "VGGT_PATH = os.path.join(REPO_ROOT, \"external\", \"VGGT\")\n",
    "if VGGT_PATH not in sys.path:\n",
    "    sys.path.insert(0, VGGT_PATH)\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Device: prefer MPS (Apple Silicon), then CUDA, then CPU\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"torch: {torch.__version__}\")\n",
    "print(f\"device: {device}\")\n",
    "print(f\"VGGT on path: {os.path.exists(os.path.join(VGGT_PATH, 'vggt'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Model Load & Parameter Counts <a id='2'></a>\n",
    "\n",
    "VGGT consists of:\n",
    "- **Aggregator** — DINOv2-style ViT-L backbone + cross-frame attention\n",
    "- **CameraHead** — 4-iteration refinement of [T | quat | FoV] from camera tokens\n",
    "- **DPTHead (depth)** — dense depth + confidence, output_dim=2\n",
    "- **DPTHead (point)** — dense 3D world points + confidence, output_dim=4\n",
    "- **TrackHead** — point tracking (skipped here; requires query_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucamac/Coding/Master-Thesis-3D-VLA/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading VGGT-1B from HuggingFace Hub...\n"
     ]
    }
   ],
   "source": [
    "from vggt.models.vggt import VGGT\n",
    "\n",
    "# Download from HuggingFace Hub (~4 GB, cached after first run)\n",
    "print(\"Loading VGGT-1B from HuggingFace Hub...\")\n",
    "model = VGGT.from_pretrained(\"facebook/VGGT-1B\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print(\"Model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(module):\n",
    "    if module is None:\n",
    "        return 0\n",
    "    return sum(p.numel() for p in module.parameters())\n",
    "\n",
    "total = count_params(model)\n",
    "components = {\n",
    "    \"aggregator\":   count_params(model.aggregator),\n",
    "    \"camera_head\":  count_params(model.camera_head),\n",
    "    \"depth_head\":   count_params(model.depth_head),\n",
    "    \"point_head\":   count_params(model.point_head),\n",
    "    \"track_head\":   count_params(model.track_head),\n",
    "}\n",
    "\n",
    "print(f\"{'Component':<20} {'Params':>12} {'Share':>8}\")\n",
    "print(\"-\" * 42)\n",
    "for name, n in components.items():\n",
    "    print(f\"{name:<20} {n:>12,} {n/total*100:>7.1f}%\")\n",
    "print(\"-\" * 42)\n",
    "print(f\"{'TOTAL':<20} {total:>12,} {'100.0%':>8}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Load Example Images <a id='3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from vggt.utils.load_fn import load_and_preprocess_images\n",
    "\n",
    "# Use kitchen scene (5 views)\n",
    "img_dir = os.path.join(VGGT_PATH, \"examples\", \"kitchen\", \"images\")\n",
    "img_paths = sorted(glob.glob(os.path.join(img_dir, \"*.png\")))[:5]\n",
    "print(f\"Found {len(img_paths)} images: {[os.path.basename(p) for p in img_paths]}\")\n",
    "\n",
    "# Preprocess: resize to 518px wide, center-crop, normalize to [0,1]\n",
    "images = load_and_preprocess_images(img_paths, mode=\"crop\")  # [S, 3, H, W]\n",
    "print(f\"Images tensor shape: {images.shape}  dtype: {images.dtype}\")\n",
    "print(f\"Value range: [{images.min():.3f}, {images.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = images.shape[0]\n",
    "fig, axes = plt.subplots(1, S, figsize=(4 * S, 3))\n",
    "for i, ax in enumerate(axes):\n",
    "    img_np = images[i].permute(1, 2, 0).numpy()\n",
    "    ax.imshow(img_np)\n",
    "    ax.set_title(os.path.basename(img_paths[i]))\n",
    "    ax.axis(\"off\")\n",
    "plt.suptitle(\"Kitchen scene — 5 views\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Full Forward Pass <a id='4'></a>\n",
    "\n",
    "VGGT forward signature:\n",
    "```python\n",
    "predictions = model(images)  # images: [S, 3, H, W]  →  adds batch dim internally\n",
    "```\n",
    "Output keys (all tensors have batch dim B=1):\n",
    "| Key | Shape | Description |\n",
    "|-----|-------|-------------|\n",
    "| `pose_enc` | [B, S, 9] | absT + quaR + FoV (last iter) |\n",
    "| `pose_enc_list` | list[4] of [B, S, 9] | all 4 CameraHead iterations |\n",
    "| `depth` | [B, S, H, W, 1] | predicted depth |\n",
    "| `depth_conf` | [B, S, H, W] | depth confidence |\n",
    "| `world_points` | [B, S, H, W, 3] | 3D world coords |\n",
    "| `world_points_conf` | [B, S, H, W] | point confidence |\n",
    "| `images` | [B, S, 3, H, W] | input images (inference only) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "images_dev = images.to(device)\n",
    "\n",
    "t0 = time.time()\n",
    "with torch.no_grad():\n",
    "    predictions = model(images_dev)\n",
    "elapsed = time.time() - t0\n",
    "\n",
    "print(f\"Inference time: {elapsed:.2f}s  ({elapsed/S:.2f}s per frame)\\n\")\n",
    "print(f\"{'Key':<25} {'Shape':>30}\")\n",
    "print(\"-\" * 57)\n",
    "for k, v in predictions.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        print(f\"{k:<25} {str(tuple(v.shape)):>30}\")\n",
    "    elif isinstance(v, list):\n",
    "        print(f\"{k:<25} {'list[' + str(len(v)) + '] of ' + str(tuple(v[0].shape)):>30}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Camera pose encoding breakdown: [T(3) | quat(4) | FoV_h FoV_w(2)]\n",
    "pose = predictions[\"pose_enc\"]  # [1, S, 9]\n",
    "print(\"Camera pose encodings (frame 0):\")\n",
    "print(f\"  T   (translation): {pose[0, 0, :3].cpu().numpy()}\")\n",
    "print(f\"  quat (rotation):   {pose[0, 0, 3:7].cpu().numpy()}\")\n",
    "print(f\"  FoV_h, FoV_w (rad): {pose[0, 0, 7:].cpu().numpy()}\")\n",
    "print(f\"  FoV_h, FoV_w (deg): {torch.rad2deg(pose[0, 0, 7:]).cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Depth & Point Cloud Visualization <a id='5'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depth maps + confidence per frame\n",
    "# depth: [B, S, H, W, 1]  depth_conf: [B, S, H, W]\n",
    "depth = predictions[\"depth\"].squeeze(-1).cpu()       # [1, S, H, W]\n",
    "depth_conf = predictions[\"depth_conf\"].cpu()         # [1, S, H, W]\n",
    "\n",
    "fig, axes = plt.subplots(2, S, figsize=(4 * S, 6))\n",
    "for i in range(S):\n",
    "    d = depth[0, i].numpy()\n",
    "    c = depth_conf[0, i].numpy()\n",
    "\n",
    "    im0 = axes[0, i].imshow(d, cmap=\"plasma\")\n",
    "    axes[0, i].set_title(f\"Depth {i}\")\n",
    "    axes[0, i].axis(\"off\")\n",
    "    plt.colorbar(im0, ax=axes[0, i], fraction=0.046)\n",
    "\n",
    "    im1 = axes[1, i].imshow(c, cmap=\"viridis\", vmin=0)\n",
    "    axes[1, i].set_title(f\"Conf {i}\")\n",
    "    axes[1, i].axis(\"off\")\n",
    "    plt.colorbar(im1, ax=axes[1, i], fraction=0.046)\n",
    "\n",
    "plt.suptitle(\"Depth maps (top) and confidence (bottom)\", y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"depth: {depth.shape}  depth_conf: {depth_conf.shape}\")\n",
    "print(f\"Depth matches image H,W: {tuple(depth.shape[-2:]) == tuple(images.shape[-2:])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D point cloud — subsample 5k points from all frames, colored by confidence\n",
    "# world_points: [B, S, H, W, 3]  world_points_conf: [B, S, H, W]\n",
    "pts3d = predictions[\"world_points\"].cpu()         # [1, S, H, W, 3]\n",
    "pts_conf = predictions[\"world_points_conf\"].cpu() # [1, S, H, W]\n",
    "\n",
    "pts_flat = pts3d[0].reshape(-1, 3).numpy()        # [S*H*W, 3]\n",
    "conf_flat = pts_conf[0].reshape(-1).numpy()       # [S*H*W]\n",
    "\n",
    "# Subsample 5k points (uniform random)\n",
    "rng = np.random.default_rng(42)\n",
    "idx = rng.choice(len(pts_flat), size=min(5000, len(pts_flat)), replace=False)\n",
    "pts_sub = pts_flat[idx]\n",
    "conf_sub = conf_flat[idx]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "sc = ax.scatter(pts_sub[:, 0], pts_sub[:, 1], pts_sub[:, 2],\n",
    "                c=conf_sub, cmap=\"viridis\", s=1, alpha=0.7)\n",
    "plt.colorbar(sc, ax=ax, label=\"confidence\", shrink=0.6)\n",
    "ax.set_title(\"World-space point cloud (5k subsample, colored by confidence)\")\n",
    "ax.set_xlabel(\"X\"); ax.set_ylabel(\"Y\"); ax.set_zlabel(\"Z\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Camera frustum plot — visualize camera positions across all frames\n",
    "from vggt.utils.pose_enc import pose_encoding_to_extri_intri\n",
    "\n",
    "H_img, W_img = images.shape[-2], images.shape[-1]\n",
    "extrinsics, intrinsics = pose_encoding_to_extri_intri(\n",
    "    predictions[\"pose_enc\"].cpu(), image_size_hw=(H_img, W_img)\n",
    ")  # extrinsics: [1, S, 3, 4]\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "# Plot point cloud (low density)\n",
    "idx2 = rng.choice(len(pts_flat), size=500, replace=False)\n",
    "ax.scatter(*pts_flat[idx2].T, c=\"lightgray\", s=0.5, alpha=0.4)\n",
    "\n",
    "# Plot camera centers\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, S))\n",
    "for i in range(S):\n",
    "    # Camera center in world space: C = -R^T @ t\n",
    "    R = extrinsics[0, i, :3, :3].numpy()  # [3, 3]\n",
    "    t = extrinsics[0, i, :3, 3].numpy()   # [3]\n",
    "    C = -R.T @ t                           # world-space camera center\n",
    "    # Camera optical axis direction (column 2 of R^T)\n",
    "    fwd = R.T[:, 2]\n",
    "    ax.scatter(*C, color=colors[i], s=60, zorder=5, label=f\"cam {i}\")\n",
    "    ax.quiver(*C, *fwd * 0.2, color=colors[i], linewidth=1.5)\n",
    "\n",
    "ax.set_title(\"Camera positions + optical axes\")\n",
    "ax.legend(loc=\"upper left\", fontsize=8)\n",
    "ax.set_xlabel(\"X\"); ax.set_ylabel(\"Y\"); ax.set_zlabel(\"Z\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. DPT Head Deep-Dive <a id='6'></a>\n",
    "\n",
    "### Architecture summary\n",
    "\n",
    "The DPT head fuses **4 intermediate ViT layers** (indices 4, 11, 17, 23) into a dense prediction:\n",
    "\n",
    "```\n",
    "aggregated_tokens[layer_idx]  →  LayerNorm  →  reshape [BS, C, ph, pw]\n",
    "  └─ projects[i]:  Conv2d(2048 → oc_i, 1×1)   # project to lower dim\n",
    "     └─ pos_embed +\n",
    "        resize_layers[i]:  ConvTranspose2d ×4 / ×2 / Identity / Conv↓2\n",
    "           └─ scratch.layerN_rn: Conv2d(oc_i → 256, 3×3)\n",
    "\n",
    "refinenet4(layer4_rn)               → upsample ×2\n",
    "refinenet3(^, layer3_rn)            → upsample ×2\n",
    "refinenet2(^, layer2_rn)            → upsample ×2\n",
    "refinenet1(^, layer1_rn)            → upsample ×2\n",
    "  → output_conv1: Conv2d(256 → 128, 3×3)\n",
    "  → interpolate to (H, W)\n",
    "  → output_conv2: Conv2d(128→32→output_dim, 3×3 → 1×1)\n",
    "  → activate_head: split → value activation + confidence activation\n",
    "```\n",
    "\n",
    "For `depth_head`: `output_dim=2`, activation=`exp`, conf_activation=`expp1`  \n",
    "For `point_head`: `output_dim=4`, activation=`inv_log`, conf_activation=`expp1`\n",
    "\n",
    "Intermediate layer indices and their upsampling:\n",
    "| idx | layers[idx] | resize | scale vs patch grid |\n",
    "|-----|-------------|--------|---------------------|\n",
    "| 4   | early       | ×4 ConvTranspose | 4× (finest) |\n",
    "| 11  | mid-early   | ×2 ConvTranspose | 2× |\n",
    "| 17  | mid-late    | Identity         | 1× |\n",
    "| 23  | late        | Conv stride 2 downsampled | 0.5× (coarsest) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register hooks on DPT depth_head intermediate projections to capture shapes\n",
    "hook_outputs = {}\n",
    "\n",
    "def make_hook(name):\n",
    "    def hook(module, inp, out):\n",
    "        hook_outputs[name] = out.detach().cpu()\n",
    "    return hook\n",
    "\n",
    "handles = []\n",
    "dh = model.depth_head\n",
    "for i, proj in enumerate(dh.projects):\n",
    "    h = proj.register_forward_hook(make_hook(f\"project_{i}\"))\n",
    "    handles.append(h)\n",
    "for i, rl in enumerate(dh.resize_layers):\n",
    "    h = rl.register_forward_hook(make_hook(f\"resize_{i}\"))\n",
    "    handles.append(h)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _ = model(images_dev)\n",
    "\n",
    "# Remove hooks\n",
    "for h in handles:\n",
    "    h.remove()\n",
    "\n",
    "print(\"DPT depth_head intermediate feature shapes (B*S, C, H, W):\")\n",
    "print(f\"{'Name':<15} {'Shape':>25}\")\n",
    "print(\"-\" * 42)\n",
    "for name in sorted(hook_outputs):\n",
    "    print(f\"{name:<15} {str(tuple(hook_outputs[name].shape)):>25}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA of patch tokens from the last aggregator layer → RGB visualization\n",
    "# Shows what the backbone has learned before the DPT head\n",
    "\n",
    "# Capture aggregated_tokens_list AND patch_start_idx from aggregator forward\n",
    "agg_tokens_captured = []\n",
    "orig_forward = model.aggregator.forward\n",
    "\n",
    "def patched_agg_forward(*args, **kwargs):\n",
    "    result = orig_forward(*args, **kwargs)\n",
    "    agg_tokens_captured.append(result)  # (aggregated_tokens_list, patch_start_idx)\n",
    "    return result\n",
    "\n",
    "model.aggregator.forward = patched_agg_forward\n",
    "with torch.no_grad():\n",
    "    _ = model(images_dev)\n",
    "model.aggregator.forward = orig_forward\n",
    "\n",
    "agg_tokens_list, patch_start_idx = agg_tokens_captured[0]\n",
    "print(f\"Number of layers captured: {len(agg_tokens_list)}\")\n",
    "print(f\"patch_start_idx: {patch_start_idx}  (1 camera token + {patch_start_idx-1} register tokens)\")\n",
    "print(f\"Token tensor shape (last layer): {agg_tokens_list[-1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA of patch tokens from the final layer → RGB visualization per frame\n",
    "# patch_start_idx = 5: camera token at 0, then 4 register tokens, then patch tokens\n",
    "\n",
    "tokens_last = agg_tokens_list[-1]  # [1, S, N_tokens, C]\n",
    "_, S_t, N, C = tokens_last.shape\n",
    "H_p = images.shape[-2] // 14\n",
    "W_p = images.shape[-1] // 14\n",
    "patch_tokens = tokens_last[0, :, patch_start_idx:patch_start_idx + H_p * W_p, :]  # [S, H_p*W_p, C]\n",
    "\n",
    "print(f\"patch_tokens shape: {patch_tokens.shape}  (expected [{S_t}, {H_p*W_p}, {C}])\")\n",
    "\n",
    "# Flatten all frames for PCA\n",
    "X = patch_tokens.reshape(-1, C).float().numpy()  # [S*H_p*W_p, C]\n",
    "X -= X.mean(0, keepdims=True)\n",
    "\n",
    "# SVD for top-3 components\n",
    "U, s, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "pca_feats = (X @ Vt[:3].T)  # [S*H_p*W_p, 3]\n",
    "\n",
    "# Normalize to [0, 1] per component\n",
    "for j in range(3):\n",
    "    mn, mx = pca_feats[:, j].min(), pca_feats[:, j].max()\n",
    "    pca_feats[:, j] = (pca_feats[:, j] - mn) / (mx - mn + 1e-8)\n",
    "\n",
    "pca_frames = pca_feats.reshape(S_t, H_p, W_p, 3)\n",
    "\n",
    "fig, axes = plt.subplots(1, S_t, figsize=(4 * S_t, 3))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(pca_frames[i])\n",
    "    ax.set_title(f\"PCA-RGB frame {i}\")\n",
    "    ax.axis(\"off\")\n",
    "plt.suptitle(\"PCA of final-layer patch tokens → RGB (top-3 PCs)\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f\"Explained variance ratio (top-3): {s[:3]**2 / (s**2).sum() * 100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. DPT Disabled Experiment <a id='7'></a>\n",
    "\n",
    "VGGT supports per-head enable flags at construction time:\n",
    "```python\n",
    "VGGT(enable_camera=True, enable_point=False, enable_depth=False, enable_track=False)\n",
    "```\n",
    "Here we simulate disabling depth + point heads post-hoc by setting them to `None`,\n",
    "which matches what the constructor would produce and exercises the same `if head is not None` branches in `forward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, time\n",
    "\n",
    "# Clone weights to a camera-only model (no DPT heads)\n",
    "model_no_dpt = VGGT.from_pretrained(\"facebook/VGGT-1B\")\n",
    "model_no_dpt.depth_head = None\n",
    "model_no_dpt.point_head = None\n",
    "model_no_dpt.track_head = None\n",
    "model_no_dpt = model_no_dpt.to(device)\n",
    "model_no_dpt.eval()\n",
    "\n",
    "print(f\"Full model   params: {count_params(model):>15,}\")\n",
    "print(f\"No-DPT model params: {count_params(model_no_dpt):>15,}\")\n",
    "print(f\"Removed:             {count_params(model) - count_params(model_no_dpt):>15,} \"\n",
    "      f\"({(1 - count_params(model_no_dpt)/count_params(model))*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference comparison\n",
    "N_runs = 3\n",
    "\n",
    "times_full = []\n",
    "for _ in range(N_runs):\n",
    "    t0 = time.time()\n",
    "    with torch.no_grad():\n",
    "        preds_full = model(images_dev)\n",
    "    times_full.append(time.time() - t0)\n",
    "\n",
    "times_no_dpt = []\n",
    "for _ in range(N_runs):\n",
    "    t0 = time.time()\n",
    "    with torch.no_grad():\n",
    "        preds_no_dpt = model_no_dpt(images_dev)\n",
    "    times_no_dpt.append(time.time() - t0)\n",
    "\n",
    "print(f\"Full model avg time:    {np.mean(times_full):.3f}s ± {np.std(times_full):.3f}s\")\n",
    "print(f\"No-DPT model avg time:  {np.mean(times_no_dpt):.3f}s ± {np.std(times_no_dpt):.3f}s\")\n",
    "print(f\"Speedup: {np.mean(times_full) / np.mean(times_no_dpt):.2f}×\")\n",
    "\n",
    "print(\"\\nFull model output keys:   \", list(preds_full.keys()))\n",
    "print(\"No-DPT model output keys: \", list(preds_no_dpt.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What would be needed to replace DPT with a custom head?\n",
    "\n",
    "The DPT head receives `aggregated_tokens_list` — a Python list of 24 tensors, each `[B, S, N_tokens, 2048]`, one per transformer layer.  \n",
    "It selects layers `[4, 11, 17, 23]`, extracts patch tokens `[:, :, patch_start_idx:]`, reshapes to spatial `[BS, C, H_p, W_p]`, and runs multi-scale fusion.\n",
    "\n",
    "To replace it with a **custom 3D head** (e.g. UNITE-style features):\n",
    "1. Replace `DPTHead` assignment in `VGGT.__init__` with your custom `nn.Module`.\n",
    "2. Match the call signature: `head(aggregated_tokens_list, images=images, patch_start_idx=patch_start_idx)` returning `(preds, conf)`.\n",
    "3. The backbone (`aggregator`) and `camera_head` remain frozen during initial experiments.\n",
    "4. Alternatively, use `feature_only=True` on the existing DPT head to get 256-d spatial features, then add a thin prediction head on top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Camera Pose Encoding <a id='8'></a>\n",
    "\n",
    "VGGT uses `pose_encoding_type = \"absT_quaR_FoV\"`:  \n",
    "A 9-dimensional vector per camera:\n",
    "```\n",
    "[T_x, T_y, T_z,  q_w, q_x, q_y, q_z,  FoV_h, FoV_w]\n",
    "  translation       quaternion (WXYZ)   field-of-view (rad)\n",
    "```\n",
    "\n",
    "The **CameraHead** iteratively refines this in 4 iterations using:\n",
    "- `embed_pose`: linear projection 9→2048\n",
    "- `poseLN_modulation`: SiLU → Linear → shift/scale/gate (AdaLN)\n",
    "- `trunk`: 4 transformer blocks on camera tokens\n",
    "- `pose_branch`: MLP 2048→9 (delta update)\n",
    "- Final: `activate_pose` applies per-component activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round-trip sanity check: encode → decode → compare\n",
    "from vggt.utils.pose_enc import extri_intri_to_pose_encoding, pose_encoding_to_extri_intri\n",
    "\n",
    "# Construct a synthetic camera: identity rotation, translation [1,2,3], 60° FoV\n",
    "B_t, S_t = 1, 1\n",
    "H_t, W_t = 480, 640\n",
    "\n",
    "# Extrinsics [B, S, 3, 4]: [R | t]\n",
    "extrinsics_gt = torch.zeros(B_t, S_t, 3, 4)\n",
    "extrinsics_gt[0, 0, :3, :3] = torch.eye(3)      # identity rotation\n",
    "extrinsics_gt[0, 0, :, 3] = torch.tensor([1.0, 2.0, 3.0])  # translation\n",
    "\n",
    "# Intrinsics [B, S, 3, 3]: fx=fy for 60° FoV\n",
    "fov_rad = torch.deg2rad(torch.tensor(60.0))\n",
    "f = (H_t / 2) / torch.tan(fov_rad / 2)\n",
    "intrinsics_gt = torch.zeros(B_t, S_t, 3, 3)\n",
    "intrinsics_gt[0, 0, 0, 0] = f           # fx\n",
    "intrinsics_gt[0, 0, 1, 1] = f           # fy\n",
    "intrinsics_gt[0, 0, 0, 2] = W_t / 2    # cx\n",
    "intrinsics_gt[0, 0, 1, 2] = H_t / 2    # cy\n",
    "intrinsics_gt[0, 0, 2, 2] = 1.0\n",
    "\n",
    "# Encode\n",
    "pose_enc = extri_intri_to_pose_encoding(extrinsics_gt, intrinsics_gt, image_size_hw=(H_t, W_t))\n",
    "print(f\"Pose encoding: {pose_enc[0, 0].numpy()}\")\n",
    "print(f\"  T:    {pose_enc[0, 0, :3].numpy()}\")\n",
    "print(f\"  quat: {pose_enc[0, 0, 3:7].numpy()}  (identity → [1,0,0,0] in wxyz)\")\n",
    "print(f\"  FoV:  {torch.rad2deg(pose_enc[0, 0, 7:]).numpy()} deg\")\n",
    "\n",
    "# Decode\n",
    "extrinsics_rec, intrinsics_rec = pose_encoding_to_extri_intri(pose_enc, image_size_hw=(H_t, W_t))\n",
    "\n",
    "print(f\"\\nRound-trip error:\")\n",
    "print(f\"  extrinsics MAE: {(extrinsics_rec - extrinsics_gt).abs().mean():.6f}\")\n",
    "print(f\"  intrinsics MAE: {(intrinsics_rec - intrinsics_gt).abs().mean():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CameraHead iterative refinement — capture pose_enc_list\n",
    "iter_preds = predictions.get(\"pose_enc_list\")  # list of 4 tensors [1, S, 9]\n",
    "\n",
    "if iter_preds is not None:\n",
    "    n_iters = len(iter_preds)\n",
    "    # Translation magnitude across iterations for each frame\n",
    "    trans_norms = torch.stack([\n",
    "        p[0, :, :3].norm(dim=-1)  # [S]\n",
    "        for p in iter_preds\n",
    "    ]).numpy()  # [n_iters, S]\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    # Translation norm per iteration\n",
    "    for i in range(S):\n",
    "        ax1.plot(range(n_iters), trans_norms[:, i], marker=\"o\", label=f\"frame {i}\")\n",
    "    ax1.set_xlabel(\"Iteration\")\n",
    "    ax1.set_ylabel(\"|T|\")\n",
    "    ax1.set_title(\"Translation magnitude across CameraHead iterations\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # FoV convergence\n",
    "    fov_h = torch.stack([p[0, :, 7] for p in iter_preds]).numpy()  # [n_iters, S]\n",
    "    fov_w = torch.stack([p[0, :, 8] for p in iter_preds]).numpy()\n",
    "    for i in range(S):\n",
    "        ax2.plot(range(n_iters), np.degrees(fov_h[:, i]), \"--\", label=f\"FoV_h {i}\")\n",
    "        ax2.plot(range(n_iters), np.degrees(fov_w[:, i]), \"-\", label=f\"FoV_w {i}\")\n",
    "    ax2.set_xlabel(\"Iteration\")\n",
    "    ax2.set_ylabel(\"FoV (degrees)\")\n",
    "    ax2.set_title(\"FoV convergence across CameraHead iterations\")\n",
    "    ax2.legend(fontsize=7)\n",
    "    ax2.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"pose_enc_list not in predictions — re-run the forward pass above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Thesis Notes & Summary <a id='9'></a>\n",
    "\n",
    "### Architecture summary table\n",
    "\n",
    "| Component | Role | Output |\n",
    "|-----------|------|--------|\n",
    "| **Aggregator** | DINOv2 ViT-L + cross-frame attention | 24 layers × [B, S, N, 2048] |\n",
    "| **CameraHead** | 4-iter AdaLN refinement on camera token | [B, S, 9] pose_enc |\n",
    "| **DPTHead (depth)** | Multi-scale FPN fusion → depth + conf | [B, S, 1, H, W] × 2 |\n",
    "| **DPTHead (point)** | Multi-scale FPN fusion → world XYZ + conf | [B, S, 3, H, W] + [B, S, 1, H, W] |\n",
    "| **TrackHead** | Query-point tracking (not explored here) | [B, S, N, 2] |\n",
    "\n",
    "### Full vs No-DPT comparison\n",
    "\n",
    "| | Full | No-DPT (camera only) |\n",
    "|-|------|----------------------|\n",
    "| Params | ? | ? |\n",
    "| Inference time | ? | ? |\n",
    "| Peak memory | ? | ? |\n",
    "| Output keys | pose_enc, depth, world_points, images | pose_enc, images |\n",
    "\n",
    "*Fill in values after running cells above.*\n",
    "\n",
    "### Integration notes for 3D-VLA pipeline\n",
    "\n",
    "- **Geometry features for OpenVLA**: The most useful VGGT outputs are `world_points` (dense XYZ per pixel) and `depth` — these can replace or augment 2D CLIP features in the observation encoder.\n",
    "- **UNITE comparison**: VGGT gives per-frame geometry; UNITE adds semantic (CLIP + articulation) labels. The thesis combines both: VGGT for 3D structure, UNITE for semantic labeling.\n",
    "- **Disable DPT for feature reuse**: With `feature_only=True` the DPT head outputs 256-d spatial features instead of final predictions — useful as an intermediate representation to feed into a custom head.\n",
    "- **Camera head as free supervision**: The predicted camera poses can cross-validate against ground-truth HM3D trajectory data without extra annotation.\n",
    "- **Computational budget**: No-DPT backbone runs ~?× faster — consider whether to freeze VGGT entirely or fine-tune the aggregator with a smaller LR.\n",
    "\n",
    "---\n",
    "*Fill in with observations after running experiments.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master-thesis-3d-vla (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
